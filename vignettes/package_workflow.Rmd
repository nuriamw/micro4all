---
title: "Micro4all workflow"
output: 
  prettydoc::html_pretty:
  theme: hpstr
vignette: >
  %\VignetteIndexEntry{package_workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>"
)
```
## Introduction

This package is intended to record R functions we usually use in our laboratory for the *analysis of amplicon data*. The following tutorial has been created to show the main utilities of this package, as well as to make public our main workflow. We will cover several steps, including: 

* Produce an amplicon sequence variant (ASV) table, making use of [dada2](https://benjjneb.github.io/dada2/tutorial.html)^1^ package.
* Study microbial diversity ($\alpha$ and $\beta$ diversity).
* Get taxonomical profiles at _phylum_ and _genus_ level.
* Differential abundance analysis with ANCOMBC^2^ package. 
* Ordination graphics and environmental fitting of soil physicochemical parameters.

### Dataset

In this tutorial, we will use a bunch of sequences of our own. Specifically, we collected endosphere samples of 8 olive trees (Picual variety) from three different locations (let's call them location 1, location 2 and location 3) and sequenced the V3-V4 region of the _16S rRNA_ gene (with Illumina MiSeq technology) in order to study the bacterial community. These paired-end fastq files (already "demultiplexed" and without barcodes) can be download [here](). In this link you can also find a phyloseq object that will be used in environmental fitting section of this tutorial.

## Install package
```{r setup}
#devtools::install_github("nuriamw/micro4all")
library(micro4all)
```

# Produce an amplicon sequence variant (ASV) table. 
The first part of this tutorial is mainly based in the information provided by `dada2` but with our own implementations. We will end this section with an ASV table, already cleaned and ready for diversity analysis.

## Prepare data: libraries and read sequences files

```{r, collapse=TRUE}
library(dada2); packageVersion("dada2")
library(ShortRead);packageVersion("ShortRead")
library(devtools);packageVersion("devtools")
library(ggplot2);packageVersion("ggplot2")
library(tidyverse);packageVersion("tidyverse")

path <- "~/Escritorio/data_micro4all/" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)


## SORT FORWARD AND REVERSE READS SEPARETELY; forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq ##
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

## Check sequences quality

A crucial step in any sequencing analysis pipeline is quality checking and we will use several approaches. First, we will generate a table with the number of reads in each sample, just to check if anything went wrong during sequencing process. 

```{r}
#### COUNT NUMBER OF READS IN EACH SAMPLE BEFORE FILTERING ####
raw_reads_count <- NULL

for (i in 1:length(fnFs)){

  raw_reads_count <- rbind(raw_reads_count, length(ShortRead::readFastq(fnFs[i])))}

rownames(raw_reads_count)<- sample.names
colnames(raw_reads_count)<- "Number of reads"

min(raw_reads_count)

max(raw_reads_count)
```


As we can see, even the minimum number of sequences is good enough for our analysis. We can also check with a histogram the distribution of sequences length.

```{r}
#### HISTOGRAM WITH SEQUENCE LENGTH DISTRIBUTION ####

reads <- ShortRead::readFastq(fnFs)

counts= NULL
uniques <- unique(reads@quality@quality@ranges@width)

for (i in 1:length(uniques)) {
  counts<- rbind(counts,length(which(reads@quality@quality@ranges@width==uniques[i])))

}

histogram <-  cbind(uniques,counts)
colnames(histogram) <- c("Seq.length", "counts")

#Check histogram matrix
head(histogram[order(histogram[,1],decreasing = TRUE),]) #Most sequences fall in expected sequence length

# PLOT HISTOGRAM
hist(reads@quality@quality@ranges@width, main="Forward length distribution", xlab="Sequence length", ylab="Raw reads")
```

Moreover, `plotQualityProfile` function of `dada2` is very useful for a quick visualization of sequences quality profile.

```{r}
## VIEW AND SAVE QUALITY PLOT FOR FW AND RV ##
plotQualityProfile(fnFs[1:2])
plotQualityProfile(fnRs[1:2])
```


## Figaro: selection of trimming parameters

In this tutorial, we will use the [Figaro](https://github.com/Zymo-Research/figaro#figaro)^3^ tool. This tool makes the selection of trimming parameters for dada2 more objective. Sadly, Figaro has not yet been updated to lead with variations in sequence lengths. We will solve this following author's recommendation (which you can find [here](https://github.com/Zymo-Research/figaro/issues/37)). It consist on performing a pre-trimming step at about 295 bp. Let's see how it works!

```{r}
#### FIGARO ####

##FILTER READS
figFs <- file.path(path, "figaro", basename(fnFs))
figRs <- file.path(path, "figaro", basename(fnRs))

#### TRIMMING AT 295 pb ####
out.figaro <- filterAndTrim(fnFs, figFs, fnRs, figRs,
                     compress=TRUE, multithread=TRUE, truncLen=c(295,295)) #

##RUN FIGARO
figaro <- system(("python3 /home/programs/figaro/figaro/figaro.py -i ~/Escritorio/data_micro4all/figaro -o ~/Escritorio/data_micro4all/figaro -a 426 -f 17 -r 21"),
                 intern=TRUE) #path to figaro program -i path to input - path to output -a length of your amplicon without primers, -f primer forward length, -r primer reverse length


head(figaro)

```


## Cutadapt: efficient removal of primers
Now that we have run Figaro and know at which position we should make the trimming, we can follow with primer removing. This will be done with [cutapat](https://cutadapt.readthedocs.io/en/stable/)^4^ tool. Cutadapt should not be applied before Figaro because it can modify the quality profiles in which Figaro is based.

```{r}
#### IDENTIFY PRIMERS ####

FWD <- "CCTACGGGNBGCASCAG"  ## CHANGE ME to your forward primer sequence
REV <- "GACTACNVGGGTATCTAATCC"  ## CHANGE ME to your reverse primer sequence

## VERIFY PRESENCE AND ORENTATION OF PRIMERS ##
allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna),
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients

## REMOVE SEQUENCES WITH Ns BEFORE PRIMER CHECK ##
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))

filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

## COUNT THE APPEARENCE AND ORIENTATION OF PRIMERS ##
primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]),
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]),
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]),
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```

At this point, and as it is explained in [dada2 ITS tutorial](https://benjjneb.github.io/dada2/ITS_workflow.html), we found what we expected to: the FWD primer appears in the forward reads in its forward orientation, but it also show up in some reverse reads in its reverse-complement orientation.  Something similar occurs with reverse primer.

Now, let's run cutadapt. We use argument ^ for anchoring primers. Moreover, we combine it with the _--discard-untrimed argument_. In this way, we make sure that a sequence is not retained if only the reverse primer is found. Forward and reverse primers should be present in order to keep the sequences. 

```{r}

cutadapt <- "/usr/local/bin/cutadapt" #path to cutadapt 

system2(cutadapt, args = c("--version")) # Run shell commands from R

##Create path to cutadapt sequences
path.cut <- file.path(path, "cutadapt") 
if(!dir.exists(path.cut)) dir.create(path.cut)

fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))


##Produce arguments for cutadapt
FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)

# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste0("-a", " ", "^",FWD,"...", REV.RC) 

# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste0("-A"," ","^", REV, "...", FWD.RC)


# Run Cutadapt

for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2,"-m", 1, # -n 2 required to remove FWD and REV from reads
                             #-m 1 is required to remove empty sequences for plotting quality plots
                             "--discard-untrimmed",
                             "-j",0,#automatically detect number of cores
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i],# input files
                             "--report=minimal")) #Report minimal reports a summary

}
```


Let's see if cutadapt has properly removed the primers

```{r}
##CHECK PRESENCE OF PRIMERS IN CUTADAPTED SEQUENCES##
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]),
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]),
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]),
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

cutFs <- sort(list.files(path.cut, pattern = "_R1_001.fastq", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2_001.fastq", full.names = TRUE))

```

As we can see, there are still some forward primers to be found on forward reads. We should not worry about that. `Cutadapt` and `primerHits` have different ways to look for primers in the sequence. Moreover, if primers are still to be found in low quality sequences, they will probably be removed in next steps. 

It's time to apply Figaro trimming parameters!


## Filter and trim sequences
```{r}

### FILTER AND TRIM SEQUENCES ACCORDING TO FIGARO ####

## Place filtered files in filtered/ subdirectory
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, truncLen=c(279,205),
                     maxN=0, maxEE=c(3,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE, minLen=50)

```

To continue with, we will apply the main core of DADA2 package. As it is very well detailed in its tutorial, we will not make a lot of comments about it


## Learn error rates

```{r}
#### LEARN ERROR RATES ####
errF <- learnErrors(filtFs, multithread=T, verbose=1 ) #
errR <- learnErrors(filtRs, multithread=T, verbose=1) #

```

## View error plots

As we can see, the plots follow the tendency adequately. 
```{r}
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)

```

## Dereplication

```{r, collapse=TRUE}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)

# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

```

## Sample inference


```{r}
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
dadaFs[[1]]

```

## Merge paired-end reads

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)

# Inspect the merger data.frame from the first sample
head(mergers[[1]])

```

## Construct sequence table

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab) ##
```

## Remove chimeras

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

```

## Inspect ASV length and abundance

```{r}
table(nchar(getSequences(seqtab.nochim)))  #Number of ASV of each length
```

This is useful, but we also want to know the abundance of sequences for each length. Let's calculate it and make a plot

```{r}
reads.per.seqlen <- tapply(colSums(seqtab.nochim), factor(nchar(getSequences(seqtab.nochim))), sum) #number of sequences for each length
reads.per.seqlen


## Plot length distribution
table_reads_seqlen <- data.frame(length=as.numeric(names(reads.per.seqlen)), count=reads.per.seqlen)

ggplot(data=table_reads_seqlen, aes(x=length, y=count)) + geom_col()
```

According to previous information, we will choose the interval ranging from 402 to 428 pb

```{r}
seqtab.nochim <- seqtab.nochim[,nchar(colnames(seqtab.nochim)) %in% seq(402,428)]

```

## Check number of reads

At this point, it is important to check how many sequences we have lost in each step. Apart from filtering, not other step should have led to a substantial decreased in sequence number. 


```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names


#Let's apply a mutate to get percentage, making it easier to analyze.
track <- track %>%as.data.frame() %>% mutate(Perc.input=filtered*100/input,
                                             Perc.denoisedF= denoisedF*100/filtered,
                                             Perc.denoisedR= denoisedR*100/filtered,
                                             Perc.merged= merged*100/filtered,
                                             Perc.nonchim= nonchim*100/merged)

track
```

Everything is as expected, so let's keep on with classification!


## Taxonomy classification with RDP

```{r}
taxa_rdp <- assignTaxonomy(seqtab.nochim, "~/Escritorio/data_micro4all/rdp_train_set_18_H.fa", multithread=TRUE)
```

## ASV TABLE 

We will produce a raw ASV table that can be saved as `.txt`. This allows us to manipulate the data manually if needed, as well as saving a raw data before further steps are performed.

```{r}
ASV <- seqtab.nochim
ASVt <- t(ASV)

## SUBSTITUTE 'NA' WITH 'UNCLASSIFIED'and remove species column
taxa_rdp_na <- apply(taxa_rdp,2, tidyr::replace_na, "unclassified")[,-7]

##Create names for each ASV: if there are 1000 ASVs, call them from ASV0001 to ASV1000
number.digit<- nchar(as.integer(nrow(ASVt)))
names <- paste0("ASV%0", number.digit, "d") #As many 0 as digits
ASV_names<- sprintf(names, 1:nrow(ASVt))

## CREATE AND SAVE ASV TABLE
ASV_table_classified_raw<- cbind(as.data.frame(taxa_rdp_na,stringsAsFactors = FALSE),as.data.frame(ASV_names, stringsAsFactors = FALSE),as.data.frame(ASVt,stringsAsFactors = FALSE))

##Make rownames a new column, in order to keep sequences during the filtering process
ASV_seqs <- rownames(ASV_table_classified_raw)
rownames(ASV_table_classified_raw) <- NULL
ASV_table_classified_raw <- cbind(ASV_seqs, ASV_table_classified_raw)
```

## MOCK community

In order to filter out sporious ASV, we always introduce in our sequencing three samples of a commercial MOCK community (ZymoBIOMICS Microbial Community Standard II (Log Distribution), ZYMORESEARCH, CA, USA). Because we already know its composition, we can check if there is any ASV identified by the sequencing process that is not a real member of the community. Once identified, we calculate the percentage of sequences it represents. We make the assumption that ASVs that represent less than this percentage should be discarded from our data. In order to automate this analysis, `micro4all` implements a function called `MockCommunity`. Let's see how it works:

```{r message=TRUE, warning=TRUE, paged.print=TRUE}

#### REMOVE MOCK COMMUNITY ####

##GET TABLE FILERED ACCORDING TO MOCK
ASV_filtered_sorted<-MockCommunity(ASV_table_classified_raw, mock_composition, ASV_column = "ASV_names") #mock_composition is a data frame included as data in this package. See documentation for details.
```

`MockCommunity` makes a question to user before making the decision. Why? Sometimes, like in the example above, an ASV could be classified with a different name, still being the same genera. In this example, the first ASV which is not detected in the MOCK community composition table, is _Limosilactobacillus_. However, we can see that it represents a high percentage of sequences. This is probably a _bacillus_ from MOCK samples. Because of that, we decide to choose the second option.

Once the percentage is applied, we can remove sequences coming from chloroplast or mitochondria. 

```{r}
#### REMOVE CHLOROPLASTS AND MITOCHONDRIA ####
ASV_final_table<-ASV_filtered_sorted[(which(ASV_filtered_sorted$Genus!="Streptophyta"
                                            & ASV_filtered_sorted$Genus!="Chlorophyta"
                                            &ASV_filtered_sorted$Genus!="Bacillariophyta"
                                            &ASV_filtered_sorted$Family!="Streptophyta"
                                            & ASV_filtered_sorted$Family!="Chlorophyta"
                                            &ASV_filtered_sorted$Family!="Bacillariophyta"
                                            & ASV_filtered_sorted$Family!="Mitochondria"
                                            & ASV_filtered_sorted$Class!="Chloroplast"
                                            & ASV_filtered_sorted$Order!="Chloroplast"
                                            & ASV_filtered_sorted$Kingdom!="Eukaryota"
                                            & ASV_filtered_sorted$Kingdom!="unclassified")),]





##After this, I checked the presence of Cyanobacteria/Chloroplast	at phylum level.
#I only had two ASVs classified as Cyanobacteria/Chloroplast at phylum level which were not classified at deeper levels,
#so I removed them, as follow:
#

ASV_final_table<-ASV_final_table[(which(ASV_final_table$Phylum!="Cyanobacteria/Chloroplast"
)),]

```

# DIVERSITY ANALYSIS

We almost have all components to perform a full diversity analysis. For this purpose, we need a phyloseq object. Before that, we should produce a phylogenetic tree, which allows us to use _UniFrac_ and _WeightedUnifrac_ distances later on. We will use programs as MAFFT^5 and FastTreeMP^6.


```{r}
## GET THE FASTA FILE ##
library(seqinr)
ASVseqs_MOCK <- as.list(ASV_final_table$ASV_seqs)
write.fasta(ASVseqs_MOCK, names=ASV_final_table$ASV_names, file.out="ASV_tree.fas", open = "w", nbchar =1000 , as.string = FALSE)

##Get the alignment ##
mafft <- "/usr/bin/mafft"     #path to program

system2(mafft, args="--version")
system2(mafft, args=c("--auto", "ASV_tree.fas>", "alignment"))

## Get the tree ##
FastTreeMP <- "/home/programs/FastTreeMP/FastTreeMP"

system2(FastTreeMP, args="--version" )
system2(FastTreeMP, args = c("-gamma", "-nt", "-gtr", "-spr",4 ,"-mlacc", 2,  "-slownni", "<alignment>", "tree"))# Run FastTreeMP

##Introduce phylogenetic tree

tree <-  phyloseq::read_tree("tree")
```

## Create phyloseq 
We will put all the information we have (classification, abundance, metadata, ASV sequences and phylogenetic tree) in one object thanks to `phyloseq`^7. We get metadata table from `micro4all`package, as it is stored as data, but it simply consist on a data frame with sample names and location. Later on, this will allow us to group samples by location.

```{r, collapse=TRUE}
##Load packages
library(phyloseq); packageVersion("phyloseq")
library(Biostrings); packageVersion("Biostrings")
library(GUniFrac); packageVersion("GUniFrac")
library(phangorn); packageVersion("phangorn")
library(vegan); packageVersion("vegan")
library(pheatmap); packageVersion("pheatmap")
library(colorspace); packageVersion("colorspace")
```


```{r}
####CREATE PHYLOSEQ OBJECT FROM TABLES
##Get tax, otu and sequences
tax <- ASV_final_table[,2:8] #Tax

OTU <-  ASV_final_table[,9:ncol(ASV_final_table)] #ASV
colnames(OTU) <- str_replace_all(colnames(OTU),"-", "_")

dna<-Biostrings::DNAStringSet(ASV_final_table$ASV_seqs) #Sequences
names(dna)<- ASV_final_table$ASV_names

##ADD ASV NAMES
row.names(tax)<-ASV_final_table$ASV_names
row.names(OTU)<-ASV_final_table$ASV_names
#Check rownames are equal
identical(rownames(OTU), rownames(tax))

##CONVERT TO PHYLOSEQ FORMART
metadata.micro <-metadata.micro
metadata.micro$samples <- str_replace_all(metadata.micro$samples,"-", "_")
rownames(metadata.micro) <- str_replace_all(rownames(metadata.micro),"-", "_")
as.data.frame(gsub("-",replacement = ".", metadata.micro))
phy_OTUtable<-otu_table(OTU, taxa_are_rows = T)
phy_taxonomy<-tax_table(as.matrix(tax))
phy_metadata<-sample_data(metadata.micro)

##Introduce phylogenetic tree
unrooted_tree<- tree
is.rooted(unrooted_tree)

##Produce root
tree_root<-root(unrooted_tree, 1, resolve.root = T)
tree_root
is.rooted(tree_root)

##Add tree to phyloseq
location_phyloseq<-phyloseq(phy_OTUtable,phy_taxonomy,phy_metadata,dna,tree_root)

```

## $\alpha$ diversity
### Data preparation
First, we will produce a table with $\alpha$ diversity indexes. For this purpose, we will rarefy the data to the lowest sequence number. This is done in order to avoid an over estimation of diversity only because of a greater number of sequences.

```{r}
rarefaction <- rarefy_even_depth(location_phyloseq, sample.size = min(sample_sums(location_phyloseq)), rngseed = TRUE)

#Get the seed and record it for reproducible analysis
.Random.seed[1]
##[1] 10403
```


Rarefaction curves should be produced afterwards. This makes it easy to visualize if rarefaction process encountered a great loss of information or not. This plot represents number of ASV (richness) against number of sequences. Moreover, a vertical line will show the number of sequences where normalization occurred. If this line is found in the asymptote of every rarefaction curve, that means we are safe: although we would have more sequences for every sample, we would not have a significant greater number of ASV. 

```{r}
ASV <- as.data.frame(t(otu_table(location_phyloseq)))
sample_names <- rownames(ASV)

#Generate rarefaction curves
rarecurve <- rarecurve(ASV, step = 100, label = F)
```

We will make some preparation for ggplot

```{r}

#For each rarefaction curve, transform rarecurve output to a dataframe.
rarecurve_table <- lapply(rarecurve, function(x){
  b <- as.data.frame(x)
  b <- data.frame(ASV = b[,1], raw.read = rownames(b))
  b$raw.read <- as.numeric(gsub("N", "",  b$raw.read))
  return(b)
})

#Produce a column with sample names and put everything in one data frame
names(rarecurve_table) <- sample_names
rarecurve_table <- purrr::map_dfr(rarecurve_table, function(x){
  z <- data.frame(x)
  return(z)
}, .id = "Sample")


#To color lines according to group, let's create a new column
#Coloring
color <- NULL
for (i in (1:length(rarecurve_table$Sample))) {
  color <- rbind(color,metadata.micro$location[which(metadata.micro$samples==rarecurve_table$Sample[i])])##Change "Location" to variable of interest
}

#Bind this column
rarecurve_table_color <- cbind(rarecurve_table, color)
colnames(rarecurve_table_color) <- c(colnames(rarecurve_table), "Location")


## RARECURVE WITH GGPLOT ##
rareggplot<-ggplot(rarecurve_table_color, aes(x=raw.read, y=ASV, colour=Location, group=Sample)) + ### IMPORTANT TO GROUP BY SAMPLE
  theme_bw()+
  geom_point(aes(colour=Location), size=1)+
  geom_line(aes(colour=Location),size=0.5)+ #Change this for line thickness
  geom_vline(aes(xintercept = min(sample_sums(location_phyloseq))), lty=1, colour="black")+
  scale_fill_manual(values = c("location1"= "#33CC00", "location2"= "#ffd624", "location3"="#80CC88"))+
  scale_color_manual(values = c("location1"= "#33CC00","location2"= "#ffd624", "location3"="#80CC88"),
                     name="Location",
                     breaks=c("location1", "location2","location3"),
                     labels=c("Location 1","Location 2","Location 3"))+
  labs(title="Rarefaction curves", x="Number of sequences", y="Number of ASV")+
  guides(alpha="none")+
  theme(legend.key=element_blank(),
        legend.title.align = 0.85,
        legend.title = element_text(face="bold",size=14),
        axis.text = element_text(size=14),
        axis.title = element_text(size = 16),
        plot.title = element_text(hjust=0.5, face="bold", size=16),
        legend.text = element_text(size = 12))

rareggplot

```

As we can see, the rarefaction process took place in the asymptote of all samples. 

For saving plots in your machine, an easy and versatile option is to use `ggsave` form  `ggplot`. For publication, you can save it in .TIFF, but there are also other interesting formats. For example, .svg allows you to open the file in `PowerPoint` and easily modify it. If you have more knowledge of softwares as `Illustrator` you can save it in .eps

```{r}
#tiff
ggsave(filename = "rarefaction_curve.tiff", plot = rareggplot,device = tiff(),width = 18, height = 16, units = "cm", dpi = 800)

#svg
library(svglite)
ggsave(filename = "rarefaction_curve.svg", plot = rareggplot,device = svg())

#eps
postscript("curve_wito_RE.1.10.eps")
rareggplot
dev.off()  # Close the device

```


Let's continue! Now, we will compute symbol("alpha") diversity indexes.

```{r}
#Estimate alpha indixes and save it
alpha_diversity_rarefied <- estimate_richness(location_phyloseq)

## CALCULATE EVENNESS
alpha <- cbind(alpha_diversity_rarefied, metadata.micro) 
Evenness_index <- as.data.frame(alpha_diversity_rarefied$Shannon/log(specnumber(as.data.frame(t(OTU)))))
Evenness <- cbind(Evenness_index, rownames(alpha))
colnames(Evenness) <- c("Evenness", "Samples")
```

In this step, it is worth mentioning that we will not use diversity estimates of richness. These estimates rely on singletons. However, `dada2` doesn't produce singletons as ASV, they can only appear because of the merging process (being not a real singleton for a richness estimate to rely on). For more details, check [dada2 issue #92](https://github.com/benjjneb/dada2/issues/92).
```{r}
#Calculate indexes and remove estimates that rely on singletons.
indexes <- estimate_richness(rarefaction, measures=c("InvSimpson", "Shannon", "Observed"))

#Generate final table
alpha_diversity_table <- cbind(indexes, Evenness$Evenness, metadata.micro)
colnames(alpha_diversity_table)<- c("Observed", "Shannon", "InvSimpson", "Evenness",colnames(metadata.micro))

```

We got the final table with the diversity indexes we want to study, but it is also interesting to produce a table ready for publication. 

```{r}
#Select numeric columns and compute mean and SD
alpha_mean <- aggregate(alpha_diversity_table[,1:4], list(grouping=alpha_diversity_table$location), mean)%>% mutate_if(is.numeric, round, digits=2)
alpha_sd <- aggregate(alpha_diversity_table[,1:4], list(grouping=alpha_diversity_table$location), sd)%>% mutate_if(is.numeric, round, digits=2)

#Paste mean ± SD 
mean_sd <- NULL
for (i in 2:5){
  mean_sd <- cbind(mean_sd,paste0(alpha_mean[,i], " +/- ", alpha_sd[,i]))}


#generate table and give it columns names
alpha_pub_table <- cbind(alpha_mean$grouping, mean_sd)
colnames(alpha_pub_table) <- c("Location","Observed", "Shannon", "InvSimpson", "Evenness")


as.data.frame(alpha_pub_table)

```


### Statistical analysis

First, we will check the homogeneity of variances and normality of data, in order to choose a proper statistical method later on. Thanks to `levene.test.alpha` and `Shapiro` functions from `micro4all`, we can perform these tests for every index in just one step.

```{r}
library(micro4all)
library(data.table)
levene<- levene.test.alpha(alpha_diversity_table, 4, "location")
data.table(levene)


shapiro <- Shapiro(alpha_diversity_table, 4, "location")
data.table(shapiro)

```

None of them returns a significant value. That means, according to these statistics tests, every diversity index from our data follows a normal distribution with no significant differences in variance (homoscedasticity). With this information we can perform an ANOVA test. With `BalancedAnova`function from `micro4all`, it is possible to run an ANOVA test for every index at once. `BalancedAnova` returns a list with two elements, first element is a data frame with summary results for all indexes. 

```{r}
balanced_anova<- BalancedAnova(alpha_diversity_table, numberOfIndexes = 4, formula = "location")

data.table(balanced_anova[[1]])

```
Remember that, in case you have an unbalanced experiment, you can always use the function `UnbalancedAnova` in a similar way. Moreover, if you prefer to use a non-parametrical test, the function `KruskalWallis` is also available. 

Now that we have an ANOVA result, imagine we want to test differences between every two groups, that is, L1 vs L2, L2 vs L3 and L1 vs L3. Let's do that, again for every index, with `Tukey.test` from `micro4all`.

```{r}
tukey <- Tukey.test(alpha_diversity_table, 4, "location", balanced=TRUE)

tukey
```


Another pairwise test option is the Dunn's test, which is less powerful but make no assumptions on data

```{r}
dunn <- dunnT(alpha_diversity_table, 4, "location")

dunn[[2]]
```

When using `KruskalWallis` function, the pairwise test to be implemented should not be Tukey's test, because it assumes normal distribution of data. Instead, use Dunn's test or Mann–Whitney–Wilcoxon. `micro4all`also implements looping functions for these tests

```{r}
wilcoxon <- wilcoxon.test(alpha_diversity_table, 4, "location", p.adjust.method = "BH")

wilcoxon

```

### Plot $\alpha$ diversity indexes
```{r}
alpha_plot_table <- tidyr::gather(alpha_diversity_table, key = "Measure", value = "Value", Observed, Shannon, InvSimpson, Evenness)

alpha_graphic <- ggplot(data = alpha_plot_table, aes(x = location, y = Value)) +
  facet_wrap(~Measure, scale = "free") +
  geom_boxplot()+
  theme(axis.text.x = element_text(size=13),
        legend.position="bottom", strip.text = element_text(size = 20), axis.text.y = element_text(size=15), axis.title.y=element_text(size=20)) +
  scale_x_discrete(limits=c("location1",
                            "location2","location3"), breaks=c("location1",
                            "location2","location3"),   ##With breaks and labels you can change the name displayed on labels
                   labels=c("Location 1",
                            "Location 2","Location 3")) +

  aes(fill=location)+ scale_fill_manual(values = c( "location1"= "#33CC00",
                                                          "location2"= "#ffd624", "location3"="#80CC88"), na.translate=FALSE) +
  theme(legend.key.size = unit(1, 'cm')) +
  ylab("Alpha Diversity Measure")

alpha_graphic


#use ggsave for saving ggplots
#ggsave("alpha_plot.tiff", plot = alpha_graphic,width = 17, height = 30, units = "cm", dpi = 800 )


```

Both statistical analysis and graphic show no significant differences between groups. Yet there could be distinct groups when studying symbol(beta) diversity, i.e. integrating abundance and taxonomic information. 

## $\beta$ diversity analysis
### Data preparation

For $\beta$ diversity analysis, we first normalize data making use of `edgeR`^8 package. This is implemented to account for differences in library sizes between samples, as well as for compositionality. 

```{r}
library(edgeR); packageVersion("edgeR")
edgeR <- DGEList(counts = OTU, samples = metadata.micro, genes = tax)
edgeR <- calcNormFactors(edgeR)

##EXTRACT NORMALIZED COUNTS
ASV_norm <- cpm(edgeR, normalized.lib.sizes=T, log=F)

##CREATE NORMALIZED PHYLOSEQ OBJECT
phy_OTU_norm<-otu_table(as.data.frame(ASV_norm,row.names=F), taxa_are_rows = T)
phy_taxonomy_norm<-tax_table(as.matrix(tax))
phy_metadata_norm<-sample_data(metadata.micro)

##Add taxa names
taxa_names(phy_OTU_norm)<- taxa_names(phy_taxonomy_norm)
#Check
identical(rownames(ASV_norm), rownames(tax))

##Merge
normalized_phyloseq<-phyloseq(phy_OTU_norm,phy_taxonomy_norm,phy_metadata_norm,tree_root)

```

Once we have a normalized phyloseq object, it is possible to analyze $\beta$ diversity. A distance method will give us a measure of dissimilarities between replicates. Then, PERMANOVA tests wheter differences between groups are significant or not. We will make use of `Permanova` function from `micro4all`, in order to implement different kind of distance measures at once. 

```{r}
permanova<- Permanova(normalized_phyloseq,distances = c("bray", "unifrac", "wunifrac"), formula = "location")


permanova[[1]]
```
Bray-Curtis distance seems to explained the most variance (expressed as R^2 in the above table). Nevertheless, another test is necessary to confirm no differences are found between dispersion. If significant differences are to be found with a betadisper test, then differences found with PERMANOVA could be only due to distinct dispersion and further graphic representation would be needed. 

Again, we can loop over distances measures thanks to `micro4all`


```{r}
betadisper<- Betadispersion(location_phyloseq,distances = c("bray", "unifrac", "wunifrac"), formula = "location")
betadisper
```
In this example, dispersion was not statistically significant different between the three groups.

But, which groups are distinct? We can make a `pairwiseAdonis` test to check for differences. Again, `micro4all`gives us the option to loop over distances

```{r}
pairwise<- PairwiseAdonisFun(normalized_phyloseq,distances = c("bray", "unifrac", "wunifrac"), formula = "location", pval=0.05)


pairwise
```

It seems that, according to statistical analysis used and with Bray-Curtis distances, all groups are grouped distinctively. 
For visualization, we can use PCoA and NMDS with every measure. Here it is a loop for producing every possible graphic. It assigns each graphic to elements of a list. 

```{r}
var_list <-  c("bray", "unifrac", "wunifrac")
plot_type <-  c("PCoA", "NMDS")
combination_plot <- purrr::cross2(plot_type,var_list )


# Make plots.
plot_list = list()

for (i in 1:length(combination_plot)){
      pcoa <- ordinate(normalized_phyloseq,combination_plot[[i]][[1]],combination_plot[[i]][[2]])
      plot = plot_ordination(normalized_phyloseq, pcoa, type="samples", color="location")+
        geom_text(aes(label=location), hjust=0, vjust=0, show.legend=FALSE)+
        geom_point(size=4)
      if (combination_plot[[i]][[1]]=="NMDS"){
        plot= plot + xlab(paste("Axis 1"))+
        ylab(paste("Axis 2"))+
          theme(legend.position="bottom", plot.title = element_text(face="bold", hjust = 0.5), legend.title = element_blank(), legend.key = element_blank(),
                panel.border = element_rect(linetype = "solid", fill = NA),
                panel.background = element_rect(fill="white", colour="white"),
                panel.grid.major = element_line(colour="aliceblue", size=0.4),
                panel.grid.minor= element_line(colour = "aliceblue", size =0.4))+


          scale_color_manual(values=c("location1"= "#33CC00",
                                                          "location2"= "#ffd624", "location3"="#80CC88"))+
          guides(color=guide_legend(nrow=2,byrow=TRUE))+
          guides(shape=guide_legend(nrow=2,byrow=TRUE))+
          theme(plot.title = element_text(face="bold", hjust = 0.5))+
          ggtitle(paste("Bacterial Community", combination_plot[[i]][[1]], "on", combination_plot[[i]][[2]], "distances", round(pcoa$stress, digits = 3)))

        plot_list[[i]] = plot


      }
      else {
        plot= plot +   xlab(paste("PCo 1", paste("(",round(pcoa$values[1,2]*100,1),"%",")",sep=""),sep=" "))+
          ylab(paste("PCo 2", paste("(",round(pcoa$values[2,2]*100,1),"%",")",sep=""),sep=" "))+

          theme(legend.position="bottom", plot.title = element_text(face="bold", hjust = 0.5), legend.title = element_blank(), legend.key = element_blank(),
                panel.border = element_rect(linetype = "solid", fill = NA),
                panel.background = element_rect(fill="white", colour="white"),
                panel.grid.major = element_line(colour="aliceblue", size=0.4),
                panel.grid.minor= element_line(colour = "aliceblue", size =0.4))+


          scale_color_manual(values=c("location1"= "#33CC00",
                                                          "location2"= "#ffd624", "location3"="#80CC88"))+
          guides(color=guide_legend(nrow=2,byrow=TRUE))+
          guides(shape=guide_legend(nrow=2,byrow=TRUE))+
          theme(plot.title = element_text(face="bold", hjust = 0.5))+
          ggtitle(paste("Bacterial Community", combination_plot[[i]][[1]], "on", combination_plot[[i]][[2]], "distances"))

        plot_list[[i]] = plot


      }


       }



plot_list[[1]]

```

Going deeper in the analysis, we can get a bar plot with the variance explained by every axis. However, we should be careful in this sense because we are using non-Euclidian distances. This is why we can find negative eigenvalues (imaginary dimensions). If we want to visualize corrected eigenvalues, select element `Corr_eig` in `pcoa`.

```{r}
PCOA_Wunifrac  <-  ordinate(normalized_phyloseq, "PCoA", "wunifrac", formula="location")
length(PCOA_Wunifrac$values$Relative_eig)
barplot(100*PCOA_Wunifrac$values$Relative_eig, ylim=c(0,10+100*max(PCOA_Wunifrac$values$Relative_eig)),
        xlab=c("Axes"),ylab = c("Eigenvalue (%)"),axisnames = T,
        axis.lty = 1)
```

And with the next line, we can get the percentage of variance explained by the number of axes we choose. 

```{r}
sum(PCOA_Wunifrac$values$Relative_eig[1:10])
```

## PRODUCE TABLES WITH RELATIVE ABUNDANCE

For further analysis, or to represent data, it is useful to have tables with relative abundance at different levels. First, we usually produce a table with relative abundance by sample. 

```{r}
##CALCULATE RELAIVE ABUNDANCE
sample_relabun <- transform_sample_counts(location_phyloseq, function(x){x/sum(x)}*100)
##CONSTRUCT THE TABLE
OTU_sample <-  as.data.frame((otu_table(sample_relabun)))
taxonomy_sample <- as.data.frame(tax_table(sample_relabun))
identical(rownames(OTU_sample),rownames(taxonomy_sample))
sample_relabun_table  <- cbind(taxonomy_sample, OTU_sample)
#SORT IT
sample_relabun_table <- sample_relabun_table[sort(sample_relabun_table$ASV_names, decreasing = FALSE),]

#Check relative abundance sums 100
colSums(sample_relabun_table[,8:ncol(sample_relabun_table)])


head(sample_relabun_table)
```

Of course,  we can do it at genus level or phylum level

```{r}
#Glom phyloseq at taxonomical level

location_phyloseq_genus <- tax_glom(location_phyloseq, taxrank = "Genus")
sample_relabun_genus <- transform_sample_counts(location_phyloseq_genus, function(x){x/sum(x)}*100)
##CONSTRUCT THE TABLE
OTU_sample_genus <-  as.data.frame((otu_table(sample_relabun_genus)))
taxonomy_sample_genus  <- as.data.frame(tax_table(sample_relabun_genus)[,-7])
identical(rownames(OTU_sample_genus),rownames(taxonomy_sample_genus))
sample_table_genus  <- cbind(taxonomy_sample_genus, OTU_sample_genus)
```

Furhtermore, we would like to have all unclassifieds at genus level grouped together

```{r}
##Agglomerate unclassified in out table
sample_table_genus_unclass <- sample_table_genus %>% subset(Genus=="unclassified", select=c(7:ncol(sample_table_genus))) %>%
  colSums() %>%  t() %>% as.data.frame() %>% cbind(Kingdom="unclassified", Phylum="unclassified", Class="unclassified",
                                                   Order="unclassified", Family="unclassified", Genus="unclassified",  .)

sample_table_genus_final <- rbind(subset(sample_table_genus, Genus!="unclassified"), sample_table_genus_unclass)

#Check relative abundance sums 100
colSums(sample_table_genus_final[,8:ncol(sample_table_genus_final)])


head(sample_table_genus_final)
```


Next, we can group it by the variable of interest, in our case, location (at genus level)

```{r}
otu_genus <- sample_table_genus_final[,7:ncol(sample_table_genus_final)] %>% t() %>% as.data.frame()
#Save TAXonomy data
tax_genus <- sample_table_genus_final[,1:6]

#Calculate OTU mean abundance based on grouping factor (e.g., location)
location_mean <- aggregate(otu_genus, by=list(metadata.micro$location), FUN=mean)%>% column_to_rownames("Group.1") %>% t()
#Calculate OTU SD  based on grouping factor (e.g., location) and change colnames
location_SD <- aggregate(otu_genus, by=list(metadata.micro$location), FUN=sd)%>% column_to_rownames("Group.1")  %>% t()  %>%
  as.data.frame() %>% rename_with(.fn= ~paste0(colnames(location_mean), "SD"))
#Merge mean abundance, SD and taxonomy.
genus_location_table <- merge(tax_genus, location_mean, by=0) %>%column_to_rownames("Row.names") %>%
  merge(location_SD, by=0) %>% column_to_rownames("Row.names")

#Check abundances sum 100
colSums(genus_location_table[,7:ncol(genus_location_table)])

head(genus_location_table)

```
We can also aggregate by location at ASV level


```{r}
#Save OTU data
otu_location_ASV <- sample_relabun_table[,8:ncol(sample_relabun_table)] %>% t() %>% as.data.frame()
#Save Taxnomy data
tax_location_ASV <- sample_relabun_table[,1:7]

#Calculate OTU mean abundance based on grouping factor (e.g., PLOT)
location_ASV_mean <- aggregate(otu_location_ASV, by=list(metadata.micro$location), FUN=mean)%>% column_to_rownames("Group.1") %>% t()
#Calculate OTU SD  based on grouping factor (e.g., PLOT) and change colnames
location_ASV_SD <- aggregate(otu_location_ASV, by=list(metadata.micro$location), FUN=sd)%>% column_to_rownames("Group.1")  %>% t()  %>%
  as.data.frame() %>% rename_with(.fn= ~paste0(colnames(location_ASV_mean), "SD"))

#Merge mean abundance, SD and taxonomy.
ASV_location_table <- merge(tax_location_ASV, location_ASV_mean, by=0) %>%column_to_rownames("Row.names") %>%
  merge(location_ASV_SD, by=0) %>% column_to_rownames("Row.names")

#Check abundances sum 100
colSums(ASV_location_table[,8:ncol(ASV_location_table)])

```


## Taxonomical profile graphics

Usually, taxonomical profiles are interesting graphics to present when publishing data. They give a visual idea of the main taxonomical groups present in our comminities.

First, we prepare a table, so we can label genera representing less than a 1% of the sequences as 'other genera'. Moreover, we reorder this table in a way that graphic will represent genera, unclassified and other genera (from bottom to top).

```{r}


taxonomical_genus_location <- genus_location_table %>% select(6:9) %>% #select columns with genera and abundance
  pivot_longer(!Genus, names_to="Samples", values_to="Abundance") %>%
  mutate(Genus = case_when(Abundance<= 1.0 & Genus!="unclassified" ~ "Other genera (<1%)", TRUE ~ as.character(Genus)))

#Reorder table (first, descendant, then unclassified and other phyla)
taxonomical_genus_location <- taxonomical_genus_location %>% filter(Abundance > 1 & Genus!="unclassified") %>%
  arrange(desc(Abundance))

taxonomical_genus_location <- rbind(taxonomical_genus_location, filter(taxonomical_genus_location,Genus=="unclassified"|Genus=="Other genera (<1%)"))


head(taxonomical_genus_location)
```

Afterwards, we create an expression to introduce in ggplot, in a way that phylum and genera names are writting in italics.

```{r}
library(gdata)
#Get labels for location
location_label <- levels(as.factor(unique(taxonomical_genus_location$Samples)))

#Get unique names for genera
unique_genera <- unique(as.vector(taxonomical_genus_location$Genus))

#REORDER FACTORS LEVELS IN DATA FRAME
taxonomical_genus_location$Genus=reorder.factor(taxonomical_genus_location$Genus,new.order=rev(unique_genera))

sorted_labels_genus<- as.data.frame(unique_genera)

##CREATE AN EXPRESSION FOR GGPLOT WITH ITALICS WHEN NEEDED.
sorted_labels_ggplot <- sapply(sorted_labels_genus$unique_genera,
                                           function(x) if (x == "Other phyla (<1%)"|x == "unclassified"|x == "Other genera (<1%)")
                                           {parse(text=paste0("'", as.character(x), "'"))} else {parse(text = paste0("italic('",as.character(x), "')"))})
```

When having to color a great number of objects, `randomcoloR` package can be very useful. You can combine it with package `scales` to display the random generated colors, as well as its codes. 
 
```{r}
library(randomcoloR);packageVersion("randomcoloR")
library(scales);packageVersion("scales")
colors_genus <-  randomColor(count=length(unique_genera), hue=c("random"))
show_col(colors_genus)
```

If you want to change some of these colors, it can be easily done as follows:
```{r}
colors_genus[5] <-  "#448800"
```


In this tutorial, we will use the next palette, because it has been previously optimized to include very distinctive colors

```{r}
c21 <- c(
  "dodgerblue2", "#E31A1C", # red
  "green4",
  "#6A3D9A", # purple
  "#FF7F00", # orange
  "black", "gold1",
  "skyblue2", "#FB9A99", # lt pink
  "#CAB2D6", # lt purple
  "#FDBF6F", # lt orange
  "gray70", "khaki2",
  "maroon", "steelblue4",
  "darkturquoise", "green1", "yellow4", "yellow3",
  "darkorange4", "brown"
)
```

At last, let's plot taxonomical profile at genus level!

```{r}
ggplot_title <- "Bacterial genera by location"

ggGenera=ggplot(taxonomical_genus_location, aes(x=Samples, y=Abundance, fill=Genus, order=Genus)) + geom_bar(stat="identity", position="stack")+
  scale_fill_manual(values=c21,
                    breaks=unique_genera,
                    labels=sorted_labels_ggplot)+

  labs(y="Mean relative abundance (%)", x=NULL, fill="Genus",title=ggplot_title)+
  guides(fill = guide_legend(reverse = TRUE))+
  scale_x_discrete(limits=location_label,
                   labels=location_label)+
  scale_y_continuous(expand=c(0.01,0.01),
                     breaks=c(0,10,20,30,40,50,60,70,80,90,100),
                     labels=c("0","10", "20","30","40","50","60","70","80","90","100"),
                     limits = c(NA, 100))+
  theme_bw()+
  theme(panel.border = element_rect(colour="black"), axis.title.x=element_blank(),
        plot.title = element_text(face="bold", hjust = 0.5, size=16), axis.text = element_text(size = 14),
        axis.text.x = element_text(face="bold", size=10, angle = 45, vjust=1, hjust = 1), axis.title.y = element_text(size = 16),
        legend.key.size = unit(0.9, "cm"), legend.text = element_text(size = 12),
        legend.title = element_text(size=14, face="bold"), legend.title.align=0.5)


ggGenera


```



## ANCOM-BC ANALYSIS

There are several methods to perform differential abundance analysis on microbial community data. Among them, we choose ANCOMBC^2, because it is specifically designed for this kind of data, accounting for its compositionality. Unfortunately, when having multiple groups to compare, `ancombc` function only compares the first group against all others. This is a big drawback and requires the user to change the input `phyloseq` object for every other combination between groups. To solve this problem, `micro4all`has implemented the function `ancomloop`. Apart from looping over groups, it also returns a table with ANCOM corrected logarithmic abundances. Let's see how it works!

```{r}
library(ANCOMBC);packageVersion("ANCOMBC")
library(microbiome);packageVersion("microbiome")
library(car);packageVersion("car")
library(ggplot2);packageVersion("ggplot2")
library(gridExtra);packageVersion("gridExtra")

ANCOM_location_genus <- ancomloop(input_object_phyloseq = location_phyloseq_genus, grouping = "location", ancom.options = list(global=FALSE, struc_zero=TRUE),out.unclassified = TRUE, tax.level="Genus") #When performing ancombc at genus level, we can filter unclassified genera out with out.unclassified and tax.level arguments

head(ANCOM_location_genus[["location1"]])
```


We can filter these results to get only the significant ones and the comparison we would like to analyze


```{r}
ANCOM_location1_genus <- ANCOM_location_genus[["location1"]]
ANCOM_loc1vsloc2_genus_sig <- ANCOM_location1_genus[,1:19] %>% filter(location1vslocation2_diff,TRUE)
```

### ANCOM-BC graphic

Graphical representation of differential analysis results can be achieved with two approaches: log fold change with corrected abundances or relative abundance without correction. Here, we will show how to accomplish both graphics. For publication purposes, we rather represent both graphics in one figure. 

We will start with log fold change. First, we will bind phylum and genus name (this can be replicated when representing ASV).

```{r}
#Bind phylum name to genus name
graphic_loc1vsloc2_genus<-ANCOM_loc1vsloc2_genus_sig
for (i in 1:nrow(graphic_loc1vsloc2_genus)){
  graphic_loc1vsloc2_genus$Classification[i]<- paste(graphic_loc1vsloc2_genus$Phylum[i],graphic_loc1vsloc2_genus$Genus[i],sep="|")
  }
```

Next, we create a data frame with data (classification and log fold change) and filter out those genera where log fold change is 0 or less than a specific value. Remember, log fold change correspond to `beta` parameter in `res` element from `ancombc` function. 


```{r}
##Create the data frame for representation, with Classification, Log fold change and standard deviation
logfold_df <-  graphic_loc1vsloc2_genus %>% select(Classification, location1vslocation2_beta, location1vslocation2_SE)

colnames(logfold_df)<- c("Genus", "LogFold_location1vslocation2", "SD")

##Filter out 0 log fold change and assign name according to the direction of change
logfold_df  <-  logfold_df %>%
  filter(LogFold_location1vslocation2 != 0) %>% arrange(desc(LogFold_location1vslocation2)) %>%
  mutate(group = ifelse(LogFold_location1vslocation2 > 0, "Location1", "Location2"))

logfold_df$Genus = factor(logfold_df$Genus, levels = logfold_df$Genus)

#FILTER WHEN NEEDED. This can be used to filter the results according to log fold change level
range(logfold_df$LogFold_location1vslocation2)
logfold_df_filtered <- rbind(logfold_df[which(logfold_df$LogFold_location1vslocation2>=1.5),], logfold_df[which(logfold_df$LogFold_location1vslocation2<=-1.5),])#If not filtering, use in plotting logfold_df
```

Now that we have prepared all data, we can use `ggplot` for plotting log fold change

```{r,warning=FALSE}
waterfall_location1 <-  ggplot(data = logfold_df_filtered,
           aes(x = Genus, y = LogFold_location1vslocation2, fill = group, color = group)) +
  geom_bar(stat = "identity", width = 0.7,
           position = position_dodge(width = 0.4)) +
  geom_errorbar(aes(ymin = LogFold_location1vslocation2 - SD, ymax = LogFold_location1vslocation2 + SD), width = 0.2,
                position = position_dodge(0.05), color = "black") +
  labs(x = NULL, y = "Log fold change",
       title = "Waterfall Plot for the Location Effect") +
  theme_bw() +
  theme(legend.position = "right",
        legend.title =element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.grid.minor.y = element_blank(),
        axis.text.x = element_text(angle = 60, hjust = 1))
waterfall_location1
```

As we mentioned above, we will also produce a graphic with relative abundance, specifically, a back-to-back plot. 


```{r,warning=FALSE}
#Again, bind phylum name and genus name 
pyramidal_df<- genus_location_table
for (i in 1:nrow(pyramidal_df)){
  pyramidal_df$Classification[i]<- paste(pyramidal_df$Phylum[i],pyramidal_df$Genus[i],sep="|")
  }

#Now, transform it to ggplot format
pyramidal_loc1vsloc2 <- pyramidal_df %>% select(c(13, 7:8)) %>% #select columns with genera and abundance
  pivot_longer(!Classification, names_to="Samples", values_to="Abundance")# %>%

#Filter pyrmaidal_loc1vsloc2 to include only significat genera according to ANCOM
pyramidal_loc1vsloc2_filt <- pyramidal_loc1vsloc2[which(pyramidal_loc1vsloc2$Classification %in%logfold_df_filtered$Genus ),]

#Split table according to location
loc1pyrm <- pyramidal_loc1vsloc2_filt[which(pyramidal_loc1vsloc2_filt$Samples=="location1"),]
loc2pyrm <- pyramidal_loc1vsloc2_filt[which(pyramidal_loc1vsloc2_filt$Samples=="location2"),]


#GRAPHIC
pyramidalggplot=ggplot(data = pyramidal_loc1vsloc2_filt, mapping= aes(x = Classification, y=Abundance, fill = Samples), colour="white")+
  geom_bar(loc1pyrm, stat="identity", mapping=aes(y=-Abundance))+
  geom_bar(data=loc2pyrm, stat="identity")+
  theme_bw()+
  scale_y_continuous(expand=c(0,0), labels=abs, limits=c(-3,3), breaks=seq(-3,3,1))+
  labs(y="Relative abundance (%)")+
  theme(legend.title=element_blank(), axis.title.y=element_blank(), axis.text.y= element_text(size = 8, face="bold"),
        axis.text.x = element_text(size=14, face="bold"), axis.title.x = element_text(size=18, face="bold"), legend.key.size = unit(1.1, "cm"),legend.text = element_text(size = 16), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(),panel.grid.minor.x = element_blank())+
  coord_flip()



pyramidalggplot


```

## Constrained analysis of proximities (CAP).

Microbial communities are greatly influenced by environmental factors. Thus, it is interesting to analyze how these variables relate to community changes. This can be achieved by studying different soil physicochemical parameters and fitting them to an ordination plot. The package `vegan` includes several functions and a very useful [tutorial](https://www.mooreecology.com/uploads/2/4/2/1/24213970/vegantutor.pdf). 

Among the options offered by `vegan`^9, we have implemented them in a way it is logical for our daily studies. We will exemplify this with some data *(downloaded in this link)* from pine trees, that has been published *(maybe preprint? )*. 

We will read the phyloseq file (saved as .RDS) and get the soil physicochemical parameters from metadata

```{r}
#Read phyloseq and get relative abundance
cap_phyloseq <- readRDS(file = "cap_phyloseq")
cap_phyloseq_rel <- transform_sample_counts(cap_phyloseq,function(x){x/sum(x)}*100)

#Get ASV and metadata
ASV_cap<-as.data.frame(t(otu_table(cap_phyloseq_rel)))
metadata_cap <- as.matrix(sample_data(cap_phyloseq))
metadata_cap <- as.data.frame(metadata_cap)

metadata_cap <- metadata_cap[1:ncol(metadata_cap)-1] #Remove column Season_NucleicAcid
```

Now, we will generate two models. `CAP1` will include all variables to be part of constrained analysis, that is, it will explained the variance due to all variables, and `CAP0`, which is the opposite of `CAP1`, i.e., fully unconstrained. 

These two models will be the input for `ordistep` function. The function starts with a model with all unconstrained variables (`CAP0`) and then, in each step, it adds a new variable to the constrained model. Then, it performs a checking step. If the variables left are no longer significant, or add no more explained variance to the model, the function stops. At the end, the output will be the formula with those variables that explained most variance of data. In this way, when having multiple variables, the model can be reduced and remove those that are interdependent.

```{r}
#Compute CAP1 and CAP0 models
CAP1<-capscale(ASV_cap~Water+ln_P+SOM+N+ph_H2O+ph_KCl+CN+Na_Exch+Clay+Sand+Slime+Texture, data=metadata_cap,distance = "bray") 

CAP0<-capscale(ASV_cap~1, data=metadata_cap, distance = "bray")

#Get formula with ordistep
CAP_ordi<-ordistep(CAP0, scope=formula(CAP1))
```

Thus, we know now which variables influence our microbial community according to `ordistep`. Although pH measured in H2O is a marginal result, we will also introduce it in our model because we were also interested in studying it. With theses variables, we can use the function `envfit` to fit these environmental factors as vectors into an ordination plot and get the correlation with each ordination axis.

```{r}
ef <-  envfit (CAP1~ph_KCl+ph_H2O, data = metadata_cap, perm = 999)
#Now we can adjust pvalues
ef.adj <- ef
pvals.adj <- p.adjust (ef$vectors$pvals, method = 'BH')
ef.adj$vectors$pvals <- pvals.adj
ef.adj
```

With `plot_ordination` we can get a graphic representation of ordination and fitted vectors. In this sense, we can visualize the direction of change of environmental variables.

```{r, warning=FALSE}
#First, get distance matrix
distance_matrix <- phyloseq::distance(physeq = cap_phyloseq_rel, method = "wunifrac")

#Then, generate ordination with CAP (that is, constrained ordination to ph_KCl)
CAP_wunifrac <-  ordinate(physeq = cap_phyloseq_rel, method = "CAP",distance = distance_matrix,
                                  formula = ~ph_KCl+ph_H2O )

#Produce plot
CAP_wunifrac_plot  <- plot_ordination(physeq = cap_phyloseq_rel, ordination = CAP_wunifrac,
                                             color = "Species", axes = c(1,2)) +
  aes(shape = Species) +
  geom_point(aes(colour = Species), alpha = 1, size = 3.5) +
  scale_shape_manual(values=c("Pinaster"=16,"Other"=15),
                     breaks=c("Pinaster","Other"))+
  scale_color_manual(values = c("Pinaster"="brown", "Other"="orange"),
                     name="Host genotype",
                     breaks=c("Pinaster", "Other"),
                     labels=expression(paste("Confirmed ", italic("P. pinaster")), "Pine forest samples"))+
  guides(shape="none")+
  ggtitle("CAP on Weighted Unifrac distance")+
  theme_bw()+
  theme(legend.key=element_blank(),
        legend.title.align = 0.85,
        legend.title = element_text(face="bold"),
        axis.text = element_text(size=14),
        axis.title = element_text(size = 16),
        plot.title = element_text(hjust=0.5, face="bold"),
        legend.text = element_text(size = 16))+
  geom_hline(aes(yintercept = c(0.00)), lty=2, colour="grey")+
  geom_vline(aes(xintercept = c(0.00)), lty=2, colour="grey")

#Define arrows aesthetic and labels
arrowmat <-  vegan::scores(CAP_wunifrac, display = "bp")
arrowdf <-  data.frame(labels = rownames(arrowmat), arrowmat) 
arrow_map  <-  aes(xend = CAP1, yend = CAP2,x = 0, y = 0, shape = NULL, color = NULL, label=labels)
label_map <- aes(x = 1.1 * CAP1, y = 1.1 * CAP2, shape = NULL,color = NULL, label=labels)
arrowhead  <-  arrow(length = unit(0.02, "npc"))

#Introduce them to plot
CAP_wunifrac_plot +
  geom_segment(mapping = arrow_map, size = c(ph_KCl=1,ph_H2O=1), data = arrowdf, color = c(ph_KCl="black",ph_H2O="black"),arrow = arrowhead) +
  geom_text(mapping = label_map, size = 4,data = arrowdf, show.legend = FALSE)

```



## References

1. Callahan BJ, McMurdie PJ, Rosen MJ, Han AW, Johnson AJA, Holmes SP (2016). “DADA2:
High-resolution sample inference from Illumina amplicon data.” _Nature Methods_, *13*,
581-583. doi: 10.1038/nmeth.3869 (URL: https://doi.org/10.1038/nmeth.3869).

2. Lin H, Peddada SD (2020). “Analysis of compositions of microbiomes with bias correction.” _Nature communications_, *11*(1), 1–11. doi: 10.1038/s41467-020-17041-7, https://www.nature.com/articles/s41467-020-17041-7. 

3. FIGARO: An efficient and objective tool for optimizing microbiome rRNA gene trimming parameters
Michael M. Weinstein, Aishani Prem, Mingda Jin, Shuiquan Tang, Jeffrey M. Bhasin
bioRxiv 610394; doi: https://doi.org/10.1101/610394

4. MARTIN, Marcel. Cutadapt removes adapter sequences from high-throughput sequencing reads. EMBnet.journal, [S.l.], v. 17, n. 1, p. pp. 10-12, may 2011. ISSN 2226-6089. Available at: <http://journal.embnet.org/index.php/embnetjournal/article/view/200>. Date accessed: 06 oct. 2021. doi:https://doi.org/10.14806/ej.17.1.200. 

5. Katoh K, Standley DM. MAFFT multiple sequence alignment software version 7: improvements in performance and usability. Mol Biol Evol. 2013;30(4):772-780. doi:10.1093/molbev/mst010

6. Price MN, Dehal PS, Arkin AP. FastTree 2--approximately maximum-likelihood trees for large alignments. PLoS One. 2010;5(3):e9490. Published 2010 Mar 10. doi:10.1371/journal.pone.0009490.

7. McMurdie PJ, Holmes S. phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data. Watson M, editor. PLoS One. 2013;8: e61217. pmid:23630581 

8. Robinson MD M D S G. edgeR: a Bioconductor package for differential expression analysis of digital gene expression data. Bioinformatics. 2010;26: 139–140. pmid:19910308

9. Wagner H. Vegan: community ecology package. R package version 2.5.2–5. 2018. https://www.mcglinnlab.org/publication/2019-01-01_oksanen_vegan_2019/
